#!/bin/bash

#  A failure occurs in generate-qname-lists.R in which not all duplicates
#+ (QNAMEs with more than two entries) are removed. This is because of the way
#+ that the data is read in and analyzed in chunks; the coordinate-sorted
#+ nature keeps duplicates away from each other in different chunks (see line
#+ 192); thus, true duplicates are not recognized as duplicates if they are in
#+ different chunks

#  The below code is for working out a solution to this issue by filtering out
#+ duplicate QNAME entries from AS.txt.gz files output by 06-get-AS-per-qname.R


#  Functions ------------------------------------------------------------------
cd "/Users/kalavattam/Dropbox/UW/projects-etc/2021_kga0_4dn-mouse-cross" ||
    {
        echo "Exiting: cd failed."
        exit 1
    }

. "./bin/auxiliary/functions-preprocessing.sh"
. "./bin/auxiliary/functions-in-progress.sh"

cd "./data/files_bam" ||
    {
        echo "Exiting: cd failed."
        exit 1
    }


#  Variables ------------------------------------------------------------------
n=5000000
#  From having run 06-get-AS-per-qname.R
file="./data/files_bam/Disteche_sample_1.dedup.CAST.corrected.CAST.AS.txt.gz"
file_sample="${file/.txt.gz/.${n}.txt.gz}"
# head_10 "${file}"
# AACCAATAACAAGTCAACGGAATATAGCGGCAAGTCAGCA:1417636611 -5
# AACCAATAACAAGTCAACGGAATATAGCGGTTATAATCAA:1988125074 0
# AACCAATAACAAGTCAACGGACGCTTCGTCCGAGATAAGA:2423713332 -5
# AACCAATAACAAGTCAACGGACGCTTCGTCCTCTGCGATC:1095498111 -5
# AACCAATAACAAGTCAACGGACGTACTAGGAAGTTCGCTG:353515658  0
# AACCAATAACAAGTCAACGGACGTACTAGGAAGTTCGCTG:895780689  0
# AACCAATAACAAGTCAACGGACTCGGCATACTCTGCGATC:1138973520 -5
# AACCAATAACAAGTCAACGGAGTCATGGATATCGGCATTG:707868573  0
# AACCAATAACAAGTCAACGGAGTCGCGTCGAAGTTCGCTG:994562444  0
# AACCAATAACAAGTCAACGGAGTCGCGTCGCTCTGCGATC:662408372  -5


#  The work -------------------------------------------------------------------
#  Sample 5,000,000 lines from file
randomly_sample_lines_from_file ${n} "${file}" "${file_sample}"

#  Tally how many entries for each QNAME
tally_qnames_gzip "${file_sample}" "${file/.txt.gz/.tally.txt.gz}"
tally_qnames_gt_1_gzip "${file_sample}" "${file/.txt.gz/.tally-gt-1.txt.gz}"

#  Check the heads of the tally files
head_10 "${file/.txt.gz/.tally.txt.gz}" && echo ""
head_10 "${file/.txt.gz/.tally-gt-1.txt.gz}"

#  Cut the first field (the tallies) from *.tally-gt-1.txt.gz
zcat "${file/.txt.gz/.tally-gt-1.txt.gz}" \
| cut -f2 \
| gzip \
> "${file/.txt.gz/.to-cut.txt.gz}"

#  Filter out duplicate QNAME entries from the *.AS.txt.gz file
start="$(date +%s)"
grep -v -f \
<(zcat -df "${file/.txt.gz/.to-cut.txt.gz}") \
<(zcat -df "${file_sample}") \
| gzip \
> "${file_sample/.txt.gz/.clean.txt.gz}"
end="$(date +%s)"
calculate_run_time "${start}" "${end}" ""

#  Did it work? Yes.
count_lines_gzip "${file_sample}"  # 5000000
count_lines_gzip "${file/.txt.gz/.tally.txt.gz}"  # 4999976
count_lines_gzip "${file/.txt.gz/.tally-gt-1.txt.gz}"  # 24
count_lines_gzip "${file/.txt.gz/.to-cut.txt.gz}"  # 24
count_lines_gzip "${file_sample/.txt.gz/.clean.txt.gz}"  # 4999952

echo $(( 4999976 - 24 ))  # 4999952
echo $(( 4999976 - 4999952 ))  # 24


#  The work again -------------------------------------------------------------
#  Make the filtering-with-grep commands into a function
filter_duplicate_qnames_gzip(){
    # Using an infile single-column list of QNAMEs, exclude QNAMEs from an
    # AS.txt.gz file
    #
    # :param 1: infile from which to remove lines, including path (chr)
    # :param 2: infile containing entries to filter out, including path (chr)
    # :param 3: filtered outfile, including path (chr)
    start="$(date +%s)"

    grep -v -f <(zcat -df "${2}") <(zcat -df "${1}") | gzip > "${3}" & \
    display_spinning_icon $! \
    "Removing duplicate QNAME lines from $(basename "${1}")... "
    
    end="$(date +%s)"
    calculate_run_time "${start}" "${end}" \
    "Remove duplicate QNAME lines from $(basename "${1}")."
}


#  Put other intermediate files/outfiles in bak/
mkdir -p bak
mv -- *tally* *clean* bak/

tally_qnames_gt_1_gzip \
"${file_sample}" \
"${file/.txt.gz/.tally-gt-1.txt.gz}"

filter_duplicate_qnames_gzip \
"${file_sample}" \
"${file/.txt.gz/.to-cut.txt.gz}" \
"${file_sample/.txt.gz/.clean.txt.gz}"

#  Check to make sure there are no more duplicate entries in AS.clean.txt.gz
tally_qnames_gzip \
"${file_sample/.txt.gz/.clean.txt.gz}" \
"${file/.txt.gz/.clean.tally.txt.gz}"

tally_qnames_gt_1_gzip \
"${file_sample/.txt.gz/.clean.txt.gz}" \
"${file/.txt.gz/.clean.tally-gt-1.txt.gz}"

count_lines_gzip "${file/.txt.gz/.clean.tally.txt.gz}"  # 4999952
count_lines_gzip "${file/.txt.gz/.clean.tally-gt-1.txt.gz}"  # 0
#  It works!

#  Clean up
rm -- *to-cut* *tally* *clean*


#  Work with a full-sized file: CAST ------------------------------------------
file="./data/files_bam/Disteche_sample_1.dedup.CAST.corrected.CAST.AS.txt.gz"
tally_qnames_gzip "${file}" "${file/.txt.gz/.tally.txt.gz}"
tally_qnames_gt_1_gzip "${file}" "${file/.txt.gz/.tally-gt-1.txt.gz}"

zcat "${file/.txt.gz/.tally-gt-1.txt.gz}" | cut -f2 | gzip \
> "${file/.txt.gz/.to-cut.txt.gz}"

filter_duplicate_qnames_gzip \
"${file}" \
"${file/.txt.gz/.to-cut.txt.gz}" \
"${file/.txt.gz/.clean.txt.gz}"

count_lines_gzip "${file}"  # 36022343
count_lines_gzip "${file/.txt.gz/.tally.txt.gz}"  # 36021154
count_lines_gzip "${file/.txt.gz/.tally-gt-1.txt.gz}"  # 1189
count_lines_gzip "${file/.txt.gz/.to-cut.txt.gz}"  # 1189
count_lines_gzip "${file/.txt.gz/.clean.txt.gz}"  # 36019965

echo $(( 36022343 - 1189 ))  # 36021154
echo $(( 36022343 - 36021154 ))  # 1189
echo $(( 36022343 - 36019965 ))  # 2378
expr 2378 / 2  # 1189

#  Check to make sure there are no more duplicate entries in AS.clean.txt.gz
tally_qnames_gzip \
"${file/.txt.gz/.clean.txt.gz}" \
"${file/.txt.gz/.clean.tally.txt.gz}"  # 36019965

tally_qnames_gt_1_gzip \
"${file/.txt.gz/.clean.txt.gz}" \
"${file/.txt.gz/.clean.tally-gt-1.txt.gz}"

count_lines_gzip "${file/.txt.gz/.clean.tally.txt.gz}"  # 36019965
count_lines_gzip "${file/.txt.gz/.clean.tally-gt-1.txt.gz}"  # 0


#  Work with a full-sized file: mm10 ------------------------------------------
file="./data/files_bam/Disteche_sample_1.dedup.mm10.corrected.mm10.AS.txt.gz"
tally_qnames_gzip "${file}" "${file/.txt.gz/.tally.txt.gz}"
tally_qnames_gt_1_gzip "${file}" "${file/.txt.gz/.tally-gt-1.txt.gz}"

zcat "${file/.txt.gz/.tally-gt-1.txt.gz}" | cut -f2 | gzip \
> "${file/.txt.gz/.to-cut.txt.gz}"

filter_duplicate_qnames_gzip \
"${file}" \
"${file/.txt.gz/.to-cut.txt.gz}" \
"${file/.txt.gz/.clean.txt.gz}"

count_lines_gzip "${file}"  # 38123043
count_lines_gzip "${file/.txt.gz/.tally.txt.gz}"  # 38121722
count_lines_gzip "${file/.txt.gz/.tally-gt-1.txt.gz}"  # 1321
count_lines_gzip "${file/.txt.gz/.to-cut.txt.gz}"  # 1321
count_lines_gzip "${file/.txt.gz/.clean.txt.gz}"  # 38120399

echo $(( 38123043 - 1321 ))  # 38121722
echo $(( 38123043 - 38121722 ))  # 1321
echo $(( 38123043 - 38120399 ))  # 2644
expr 2644 / 2  # 1322 (weird...)

#  Check to make sure there are no more duplicate entries in AS.clean.txt.gz
tally_qnames_gzip \
"${file/.txt.gz/.clean.txt.gz}" \
"${file/.txt.gz/.clean.tally.txt.gz}"

tally_qnames_gt_1_gzip \
"${file/.txt.gz/.clean.txt.gz}" \
"${file/.txt.gz/.clean.tally-gt-1.txt.gz}"

count_lines_gzip "${file/.txt.gz/.clean.tally.txt.gz}"  # 38120399
count_lines_gzip "${file/.txt.gz/.clean.tally-gt-1.txt.gz}"  # 0


#  Evaluate set intersections and complements ---------------------------------
mm10="./data/files_bam/Disteche_sample_1.dedup.CAST.corrected.CAST.AS.clean.txt.gz"
CAST="./data/files_bam/Disteche_sample_1.dedup.mm10.corrected.mm10.AS.clean.txt.gz"
intersection="./data/files_bam/Disteche_sample_1.dedup.intersection.txt.gz"
complement_mm10="./data/files_bam/Disteche_sample_1.dedup.complement-mm10.txt.gz"
complement_CAST="./data/files_bam/Disteche_sample_1.dedup.complement-CAST.txt.gz"
# complement_mm10_full="./data/files_bam/Disteche_sample_1.dedup.complement-mm10.full.txt.gz"
# complement_CAST_full="./data/files_bam/Disteche_sample_1.dedup.complement-CAST.full.txt.gz"

check_file_sorted "${mm10}"  # sorted
check_file_sorted "${CAST}"  # sorted

find_set_intersection "${mm10}" "${CAST}" "${intersection}"
check_file_sorted "${intersection}"

find_set_complement "${mm10}" "${CAST}" "${complement_mm10}"
check_file_sorted "${complement_mm10}"

find_set_complement "${CAST}" "${mm10}" "${complement_CAST}"
check_file_sorted "${complement_CAST}"

count_lines_gzip "${intersection}"  # 31333675
count_lines_gzip "${complement_mm10}"  # 4686290
count_lines_gzip "${complement_CAST}"  # 6786724

echo $(( 31333675 + 4686290 ))  # 36019965
echo $(( 31333675 + 6786724 ))  # 38120399

count_lines_gzip "${mm10}"  # 36019965
count_lines_gzip "${CAST}"  # 38120399
#  It works!

head_10 "${intersection}"
# AACCAATAACAAGTCAACGGAATATAGCGGCAAGTCAGCA:1417636611 -5  0
# AACCAATAACAAGTCAACGGAATATAGCGGTTATAATCAA:1988125074 0   0
# AACCAATAACAAGTCAACGGACGCTTCGTCCGAGATAAGA:2423713332 -5  0
# AACCAATAACAAGTCAACGGACGCTTCGTCCTCTGCGATC:1095498111 -5  0
# AACCAATAACAAGTCAACGGACGTACTAGGAAGTTCGCTG:353515658  0   0
# AACCAATAACAAGTCAACGGACGTACTAGGAAGTTCGCTG:895780689  0   0
# AACCAATAACAAGTCAACGGACTCGGCATACTCTGCGATC:1138973520 -5  -5
# AACCAATAACAAGTCAACGGAGTCATGGATATCGGCATTG:707868573  0   0
# AACCAATAACAAGTCAACGGAGTCGCGTCGAAGTTCGCTG:994562444  0   0
# AACCAATAACAAGTCAACGGAGTCGCGTCGCTCTGCGATC:662408372  -5  0

head_10 "${complement_mm10}"
# AACCAATAACAAGTCAACGGCCTTCATCCGGCTCCGCTTG:1699690380
# AACCAATAACAAGTCAACGGGAATGAATAATGATTCTCGT:300995275
# AACCAATAACAAGTCAACGGGAGCTCAGCCTTATAATCAA:1165617134
# AACCAATAACAAGTCAACGGGGCAGCAGTTAAGTTCGCTG:2352957594
# AACCAATAACAAGTCAACGGGGCAGCAGTTAAGTTCGCTG:998172925
# AACCAATAACAAGTCAACGGGGCAGCAGTTATCGGCATTG:1796828347
# AACCAATAACAAGTCAACGGGGCAGCAGTTCGGCGCTCAA:1170920050
# AACCAATAACAAGTCAACGGGGCAGCAGTTGCTCCGCTTG:158709372
# AACCAATAACAAGTCAACGGGGCAGCAGTTGCTCCGCTTG:49932068
# AACCAATAACAAGTCAACGGGGCAGCAGTTTTATAATCAA:333370883

head_10 "${complement_CAST}"
# AACCAATAACAAGTCAACGGACGCTTCGTCCAAGTCAGCA:670668735
# AACCAATAACAAGTCAACGGCCTTCATCCGCAAGTCAGCA:1755578853
# AACCAATAACAAGTCAACGGGAAGAGTATTAAGTAGACTA:49751106
# AACCAATAACAAGTCAACGGGAAGAGTATTCGTTACGTTG:1429264417
# AACCAATAACAAGTCAACGGGAAGAGTATTTTATAATCAA:1218476020
# AACCAATAACAAGTCAACGGGAATGAATAATGATTCTCGT:1818770205
# AACCAATAACAAGTCAACGGGAATGAATAATGATTCTCGT:1868131721
# AACCAATAACAAGTCAACGGGAATGAATAATGATTCTCGT:634286005
# AACCAATAACAAGTCAACGGGAATGAATAATGATTCTCGT:72623252
# AACCAATAACAAGTCAACGGGAGCTCAGCCCTCTGCGATC:2423972785

zcat "${intersection}" | less

#TODO Tomorrow, work up a module for correcting the AS.txt.gz files
#TODO 1/2 Work on module on reading intersection into R for allele assignments
#TODO 2/2 based on AS
